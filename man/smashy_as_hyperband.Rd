% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/configure_smashy.R
\name{smashy_as_hyperband}
\alias{smashy_as_hyperband}
\title{Emulate the Hyperband optimizer.}
\usage{
smashy_as_hyperband(search_space, eta = 3, type = "Optimizer")
}
\arguments{
\item{search_space}{(\code{\link[paradox:ParamSet]{ParamSet}})\cr
search space for which \code{\link{OptimizerSmashy}} should be configured, has one parameter tagged 'budget'.
Must be the same search space as of the \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} being
optimized with the resulting optimizer.\cr
The budget-parameter must be in log-scale; in most cases
this means that \code{\link[=budget_to_logscale]{budget_to_logscale()}} must be called on the search space. Note that this should
be done \emph{before} \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} is created with it, see
examples.}

\item{eta}{(\code{numeric(1)})\cr
Eta-parameter of Hyperband:
Factor of budget increase and at the same time, one over fraction of configurations that survive, for
each batch evaluation. Default 3.}

\item{type}{(\code{character(1)})\cr
One of \code{"Optimizer"} or \code{"Tuner"}. What class to return: An \code{\link[bbotk:Optimizer]{Optimizer}} for
optimizing \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} objects, or a
\code{\link[mlr3tuning:Tuner]{Tuner}} for tuning \code{\link[mlr3:Learner]{mlr3::Learner}}s. Defaults to \code{"Optimizer"}.}
}
\value{
\code{\link{OptimizerSmashy}} or \code{\link{TunerSmashy}}, depending on the \code{type} argument: The configured \emph{Smashy} optimizer.
}
\description{
Create a \code{\link{OptimizerSmashy}} (or \code{\link{TunerSmashy}}) object which behaves as
the Hyperband optimizer presented by Li et al. (2018).
}
\examples{
# Define the objective to optimize
# The 'budget' here simulates averaging 'b' samples from a noisy function
objective <- ObjectiveRFun$new(
  fun = function(xs) {
    z <- exp(-xs$x^2 - xs$y^2) + 2 * exp(-(2 - xs$x)^2 - (2 - xs$y)^2)
    z <- z + rnorm(1, sd = 1 / sqrt(xs$b))
    list(Obj = z)
  },
  domain = ps(x = p_dbl(-2, 4), y = p_dbl(-2, 4), b = p_int(1)),
  codomain = ps(Obj = p_dbl(tags = "maximize"))
)

search_space_proto = objective$domain$search_space(list(
  x = to_tune(),
  y = to_tune(),
  b = to_tune(p_int(1, 3^3, tags = "budget"))
))

search_space = budget_to_logscale(search_space_proto)

# Get a new OptimInstance. Here we determine that the optimizatoin goes
# for 10 full budget evaluations (10 * 2^100)
oi <- OptimInstanceSingleCrit$new(objective,
  search_space = search_space,
  terminator = trm("gens", generations = 10)
)

hb = smashy_as_hyperband(search_space)

hb$optimize(oi)

# Evaluations done by Hyperband:
## 1st bracket:
# 1st  generation: 27 @ budget  1 -> b = log(1)  = 0
# 2nd  generation:  9 @ budget  3 -> b = log(3)  = 1.1
# 3rd  generation:  3 @ budget  9 -> b = log(9)  = 2.2
# 4th  generation:  1 @ budget 27 -> b = log(27) = 3.3
## 2nd bracket:
# 5th  generation: 13 @ budget  3 (b = 1.1)
# 6th  generation:  4 @ budget  9 (b = 2.2)
# 7th  generation:  1 @ budget 27 (b = 3.3)
## 3rd bracket:
# 8th  generation:  6 @ budget  9 (b = 2.2)
# 9th  generation:  2 @ budget 27 (b = 3.3)
## 4th bracket:
# 10th generation:  4 @ budget 27 (b = 3.3)

oi$archive$data[, .N, by = c("b", "dob")]

# full eval log:
oi$archive

}
\seealso{
Other smashy configuration functions: 
\code{\link{budget_to_logscale}()},
\code{\link{configure_smashy}()},
\code{\link{smashy_as_bohb}()}
}
\concept{smashy configuration functions}
