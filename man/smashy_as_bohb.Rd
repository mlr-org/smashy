% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/configure_smashy.R
\name{smashy_as_bohb}
\alias{smashy_as_bohb}
\title{Emulate the BOHB optimizer.}
\usage{
smashy_as_bohb(search_space, eta = 3, rho = 1/3, ns = 64, type = "Optimizer")
}
\arguments{
\item{search_space}{(\code{\link[paradox:ParamSet]{ParamSet}})\cr
search space for which \code{\link{OptimizerSmashy}} should be configured, has one parameter tagged 'budget'.
Must be the same search space as of the \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} being
optimized with the resulting optimizer.\cr
The budget-parameter must be in log-scale; in most cases
this means that \code{\link[=budget_to_logscale]{budget_to_logscale()}} must be called on the search space. Note that this should
be done \emph{before} \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} is created with it, see
examples.}

\item{eta}{(\code{numeric(1)})\cr
Eta-parameter of BOHB:
Factor of budget increase and at the same time, one over fraction of configurations that survive, for
each batch evaluation. Default 3.}

\item{rho}{(\code{numeric(1)})\cr
Rho-parameter of BOHB:
fraction of configurations sampled randomly without the aid of the surrogate model.\cr
Note that this implementation differs from the implementation by Falkner et al. (2018),
since the kernel density sampler of BOHB is used even for the randomly interleaved configurations
here, whereas Falkner et al. (2018) samples these points uniformly at random.}

\item{ns}{(\code{integer(1)})\cr
Surrogate random search rate: How many randomly sampled configurations to evaluate on the surrogate model
to choose a single configuration to evaluate on the true objective function.}

\item{type}{(\code{character(1)})\cr
One of \code{"Optimizer"} or \code{"Tuner"}. What class to return: An \code{\link[bbotk:Optimizer]{Optimizer}} for
optimizing \code{\link[bbotk:OptimInstanceSingleCrit]{OptimInstanceSingleCrit}} objects, or a
\code{\link[mlr3tuning:Tuner]{Tuner}} for tuning \code{\link[mlr3:Learner]{mlr3::Learner}}s. Defaults to \code{"Optimizer"}.}
}
\value{
\code{\link{OptimizerSmashy}} or \code{\link{TunerSmashy}}, depending on the \code{type} argument: The configured \emph{Smashy} optimizer.
}
\description{
Create a \code{\link{OptimizerSmashy}} (or \code{\link{TunerSmashy}}) object which behaves as
the BOHB optimizer presented by Falkner et al. (2018).
}
\examples{
# Define the objective to optimize
# The 'budget' here simulates averaging 'b' samples from a noisy function
objective <- ObjectiveRFun$new(
  fun = function(xs) {
    z <- exp(-xs$x^2 - xs$y^2) + 2 * exp(-(2 - xs$x)^2 - (2 - xs$y)^2)
    z <- z + rnorm(1, sd = 1 / sqrt(xs$b))
    list(Obj = z)
  },
  domain = ps(x = p_dbl(-2, 4), y = p_dbl(-2, 4), b = p_int(1)),
  codomain = ps(Obj = p_dbl(tags = "maximize"))
)

search_space_proto = objective$domain$search_space(list(
  x = to_tune(),
  y = to_tune(),
  b = to_tune(p_int(1, 2^10, tags = "budget"))
))

search_space = budget_to_logscale(search_space_proto)

# Get a new OptimInstance. Here we determine that the optimizatoin goes
# for 10 full budget evaluations (10 * 2^10)
oi <- OptimInstanceSingleCrit$new(objective,
  search_space = search_space,
  terminator = trm("budget", budget = 10 * 2^10)
)

bohb = smashy_as_bohb(search_space)

bohb$optimize(oi)

}
\seealso{
Other smashy configuration functions: 
\code{\link{budget_to_logscale}()},
\code{\link{configure_smashy}()},
\code{\link{smashy_as_hyperband}()}
}
\concept{smashy configuration functions}
