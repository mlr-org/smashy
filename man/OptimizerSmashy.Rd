% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/OptimizerSmashy.R, R/TunerSmashy.R
\name{OptimizerSmashy}
\alias{OptimizerSmashy}
\alias{TunerSmashy}
\title{Surrogate Model Asisted Synchronized Hyperband Optimizer}
\description{
Perform Surrogate Model Assisted Hyperband Optimization.

Given a population size \code{mu}, a fraction of surviving individuals \code{survival_fraction}, a number of generations \code{n} and a fidelity progression \code{F1}, \code{F1}, ..., \code{Fn},
the algorithm works as follows:
\enumerate{
\item Sample an initial design of size \code{mu} at fidelity \code{F1}
\item Kill individuals that are not in the top \code{survival_fraction} part of individuals, by performance
\item Generate new individuals using random sampling and optionally a \code{\link{Filtor}}, until \code{mu} alive individuals are present again.
\item Evaluate all alive individuals at fidelity \verb{F\{generation\}}
\item Jump to 2., until termination, possibly because \code{n} generations are reached.
}

The number of generations \code{n} is determined from the \code{\link[bbotk:OptimInstance]{OptimInstance}}'s \code{\link[bbotk:Terminator]{Terminator}} object, see \strong{Terminating}
below.

The \strong{fidelity progression} uses a specially designated "budget" parameter of the \code{\link[bbotk:OptimInstance]{OptimInstance}}, which must have a \code{"budget"} tag.
The lower limit of the budget parameter is used as \code{F0}. The upper limit is used as \code{Fn}. The budget is evaluated at equally spaced values in generations \verb{0..n},
so \code{F1} - \code{F0} = \code{F2} - \code{F1} etc. In many cases it is desirable to have a multiplicative progression of "difficulty" of the problem. In this case, it is
recommended to use a budget parameter with exponential "trafo", or one with \code{logscale = TRUE} (see example).

A \code{\link{Filtor}} can be used for filtering, see the respective class's documentation for details on algorithms. The \code{\link{FiltorSurrogateProgressive}} can be used
for progressive surrogate model filtering. \code{\link{FiltorMaybe}} can be used for random interleaving.
}
\section{Fidelity Steps}{

The number of fidelity steps can be determined through the \code{fidelity_steps} configuration parameter, or can be determined when a
\code{\link{TerminatorGenerations}} is used to determine the number of fidelity refinements that are being performed. For this, the  \code{\link[bbotk:OptimInstance]{OptimInstance}}
being optimized must contain a \code{\link{TerminatorGenerations}}, either directly (\code{inst$terminator}), or indirectly through a
\code{\link[bbotk:mlr_terminators_combo]{bbotk::TerminatorCombo}} with \verb{$any} set to \code{TRUE} (recursive \code{\link[bbotk:mlr_terminators_combo]{TerminatorCombo}} may also be used). When \code{fidelity_steps} is \code{0},
the number of generations is determined from the given \code{\link[bbotk:Terminator]{Terminator}} object and the number of fidelity refinements
is planned according to this number. Other terminators may be present in a \code{\link[bbotk:mlr_terminators_combo]{TerminatorCombo}} that may
lead to finishing the tuning process earlier.

It is possible to continue optimization runs that quit early due to other terminators. It is not recommended to change \code{fidelity_steps} (or the number of generations
when \code{fidelity_steps} is 0) between run continuations, however, unless the fidelity bounds are also adjusted, since the continuation would then have a decrease in fidelity.
}

\section{Configuration Parameters}{

\code{OptimizerSmashy}'s configuration parameters are the hyperparameters of the \code{\link{Filtor}} given to the \code{filtor} construction argument, as well as:
\itemize{
\item \code{mu} :: \code{integer(1)}\cr
Population size: Number of individuals that are sampled in the beginning, and which are re-evaluated in each fidelity step. Initialized to 2.
\item \code{survival_fraction} :: \code{numeric(1)}\cr
Fraction of the population that survives at each fidelity step. The number of newly sampled individuals is (1 - \code{survival_fraction}) * \code{mu}. Initialized to 0.5.
\item \code{sampling} :: \code{function}\cr
Function that generates the initial population, as well as new individuals to be filtered from, as a \code{\link[paradox:Design]{Design}} object. The function must have
arguments \code{param_set} and \code{n} and function like \code{\link[paradox:generate_design_random]{paradox::generate_design_random}} or \code{\link[paradox:generate_design_lhs]{paradox::generate_design_lhs}}.
This is equivalent to the \code{initializer} parameter of \code{\link[=mies_init_population]{mies_init_population()}}, see there for more information. Initialized to
\code{\link[paradox:generate_design_random]{generate_design_random()}}.
\item \code{fidelity_steps} :: \code{integer(1)}\cr
Number of fidelity steps. When it is 0, the number is determined from the \code{\link[bbotk:OptimInstance]{OptimInstance}}'s \code{\link[bbotk:Terminator]{Terminator}}. See the
section \strong{Fidelity Steps} for more details. Initialized to 0.
\item \code{synchronize_batches} :: \code{logical(1)}\cr
Whether to use synchronized batches, as opposed to "Hyperband"-like successive halfing. Initialized to \code{TRUE}.
\item \code{filter_with_max_budget} :: \code{logical(1)}\cr
Whether to perform filtering with the maximum fidelity value found in the archive, as opposed the current \code{budget_survivors}. This has only an effect when
\code{fidelity_steps} is greater than 1 and some evaluations are done when the archive already contains evaluations with greater fidelity. Initialized to \code{FALSE}.
}
}

\examples{
\donttest{
lgr::threshold("warn")

#####
# Optimizing a Function
#####

library("bbotk")

# Define the objective to optimize
# The 'budget' here simulates averaging 'b' samples from a noisy function
objective <- ObjectiveRFun$new(
  fun = function(xs) {
    z <- exp(-xs$x^2 - xs$y^2) + 2 * exp(-(2 - xs$x)^2 - (2 - xs$y)^2)
    z <- z + rnorm(1, sd = 1 / sqrt(xs$b))
    list(Obj = z)
  },
  domain = ps(x = p_dbl(-2, 4), y = p_dbl(-2, 4), b = p_int(1)),
  codomain = ps(Obj = p_dbl(tags = "maximize"))
)

search_space = objective$domain$search_space(list(
  x = to_tune(),
  y = to_tune(),
  b = to_tune(p_int(1, 2^10, logscale = TRUE, tags = "budget"))
))

# Get a new OptimInstance. Here we determine that the optimizatoin goes
# for 10 generations.
oi <- OptimInstanceSingleCrit$new(objective,
  search_space = search_space,
  terminator = trm("gens", generations = 10)
)

library("mlr3learners")
# use the 'regr.ranger' as surrogate.
# The following settings have 30 individuals in a batch, the 20 best
# of which survive, while 10 are sampled new.
# For this, 100 individuals are sampled randomly, and the top 10, according
# to the surrogate model, are used.
smashy_opt <- opt("smashy", ftr("surprog",
    surrogate_learner = mlr3::lrn("regr.ranger"),
    filter.pool_factor = 10),
  mu = 30, survival_fraction = 2/3
)
# smashy_opt$optimize performs Smashy optimization and returns the optimum
smashy_opt$optimize(oi)

#####
# Optimizing a Machine Learning Method
#####

# Note that this is a short example, aiming at clarity and short runtime.
# The settings are not optimal for hyperparameter tuning.

library("mlr3")
library("mlr3learners")
library("mlr3tuning")

# The Learner to optimize
learner = lrn("classif.xgboost")

# The hyperparameters to optimize
learner$param_set$values[c("eta", "booster")] = list(to_tune())
learner$param_set$values$nrounds = to_tune(p_int(1, 4, tags = "budget", logscale = TRUE))

# Get a TuningInstance
ti = TuningInstanceSingleCrit$new(
  task = tsk("iris"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("gens", generations = 3)
)

# use ftr("maybe") for random interleaving: only 50\% of proposed points are filtered.
smashy_tune <- tnr("smashy", ftr("maybe", p = 0.5, filtor = ftr("surprog",
    surrogate_learner = lrn("regr.ranger"),
    filter.pool_factor = 10)),
  mu = 20, survival_fraction = 0.5
)
# smashy_tune$optimize performs Smashy optimization and returns the optimum
smashy_tune$optimize(ti)

}
}
\seealso{
Other optimizers: 
\code{\link{OptimizerMies}}
}
\concept{optimizers}
\section{Super class}{
\code{\link[bbotk:Optimizer]{bbotk::Optimizer}} -> \code{OptimizerSmashy}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{filtor}}{(\code{\link{Filtor}})\cr
Filtering algorithm used.}

\item{\code{selector}}{(\code{\link{Selector}})\cr
Survival selector used between fidelity steps.}

\item{\code{param_set}}{(\code{\link[paradox:ParamSet]{ParamSet}})\cr
Configuration parameters of the optimization algorithm.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-OptimizerSmashy-new}{\code{OptimizerSmashy$new()}}
\item \href{#method-OptimizerSmashy-clone}{\code{OptimizerSmashy$clone()}}
}
}
\if{html}{\out{
<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="format"><a href='../../bbotk/html/Optimizer.html#method-Optimizer-format'><code>bbotk::Optimizer$format()</code></a></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="help"><a href='../../bbotk/html/Optimizer.html#method-Optimizer-help'><code>bbotk::Optimizer$help()</code></a></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="optimize"><a href='../../bbotk/html/Optimizer.html#method-Optimizer-optimize'><code>bbotk::Optimizer$optimize()</code></a></li>
<li><span class="pkg-link" data-pkg="bbotk" data-topic="Optimizer" data-id="print"><a href='../../bbotk/html/Optimizer.html#method-Optimizer-print'><code>bbotk::Optimizer$print()</code></a></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OptimizerSmashy-new"></a>}}
\if{latex}{\out{\hypertarget{method-OptimizerSmashy-new}{}}}
\subsection{Method \code{new()}}{
Initialize the 'OptimizerSmashy' object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OptimizerSmashy$new(filtor = FiltorProxy$new(), selector = SelectorProxy$new())}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{filtor}}{(\code{\link{Filtor}})\cr
\code{\link{Filtor}} for the filtering algorithm. Default is \code{\link{FiltorProxy}}, which exposes the operation as
a configuration parameter of the optimizer itself.\cr
The \verb{$filtor} field will reflect this value.}

\item{\code{selector}}{(\code{\link{Selector}})\cr
\code{\link{Selector}} that chooses which individuals survive to the next higher fidelity step.
Note this is potentially different from the selection done in \code{filtor} (although the common case
would be to use the same \code{\link{Selector}} in both).
The \verb{$selector} field will reflect this value.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-OptimizerSmashy-clone"></a>}}
\if{latex}{\out{\hypertarget{method-OptimizerSmashy-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{OptimizerSmashy$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
\section{Super classes}{
\code{\link[mlr3tuning:Tuner]{mlr3tuning::Tuner}} -> \code{\link[mlr3tuning:TunerFromOptimizer]{mlr3tuning::TunerFromOptimizer}} -> \code{TunerSmashy}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-TunerSmashy-new}{\code{TunerSmashy$new()}}
\item \href{#method-TunerSmashy-clone}{\code{TunerSmashy$clone()}}
}
}
\if{html}{\out{
<details open><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-format'><code>mlr3tuning::Tuner$format()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-help'><code>mlr3tuning::Tuner$help()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href='../../mlr3tuning/html/Tuner.html#method-Tuner-print'><code>mlr3tuning::Tuner$print()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerFromOptimizer" data-id="optimize"><a href='../../mlr3tuning/html/TunerFromOptimizer.html#method-TunerFromOptimizer-optimize'><code>mlr3tuning::TunerFromOptimizer$optimize()</code></a></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TunerSmashy-new"></a>}}
\if{latex}{\out{\hypertarget{method-TunerSmashy-new}{}}}
\subsection{Method \code{new()}}{
Initialize the \code{TunerSmashy} object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSmashy$new(filtor = FiltorProxy$new(), selector = SelectorProxy$new())}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{surrogate_learner}}{(\code{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr}} | \code{NULL})}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-TunerSmashy-clone"></a>}}
\if{latex}{\out{\hypertarget{method-TunerSmashy-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{TunerSmashy$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
